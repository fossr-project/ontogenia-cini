Informazioni cronologiche,1.Bench4KE fills a critical gap by providing a community reference for evaluating Competency Questions (CQs) generation tools,"2. Assuming Bench4KE is extensible to other Knowledge Engineering automation tasks, it represents a vital contribution toward advancing research in this domain",3. Bench4KE’s support for evaluation using user-provided datasets fulfills a crucial requirement for validating domain-specific use cases,4. I will use Bench4KE to evaluate my methods/tools for automatic CQ generation (and other KE automation tasks that will be supported),5. The evaluation measures provided by Bench4KE are sufficient to assess tool performance for the CQ generation task,6. Bench4KE fosters the definition of standard tasks for  advancing and systematic evaluation of KE automation,7. I am willing to participate in a KE automation challenge supported by Bench4KE as benchmarking system,8. The system is easy to use and/or the instruction to use the system are clear and exhaustive ,9. I found the graphical user interface usable and the visualization of results effective,"10. Do you think Bench4KE can be improved by computing additional metrics for the CQ generation tasks? Please, indicate which ones possibly with references and motivation ",11. Do you think the Bench4KE APIs shall be modified? How?,12. What KE automation tasks you think should be supported by Bench4KE in the future?
07/05/2025 22.59.41,4,4,4,4,3,4,Yes,3,4,"Besides Jaccard and Cosine for lexical similarity, I believe BERTScore is important as it captures semantic meaning more accurately. I also suggest adding BLEU and ROUGE as additional lexical metrics, as they are widely used in NLP tasks. To make the scores fairer for paraphrased but correct CQs, WordNet-based synonym matching could be used to enhance theses lexical metrics.","I understand this question as: ""Should the way users or external systems interact with Bench4KE be extended?"" To support this, users could have clearer control over LLM-based evaluation—such as selecting the model, customizing prompts, and setting scoring criteria. A more transparent UI showing prompt–response pairs, scoring rationale, and confidence scores would help users audit this process.",Benchmark CQ-to-SPARQL conversion tools
09/05/2025 13.41.52,5,4,5,4,3,4,Yes,3,4,"Yes, Bench4KE can certainly be improved by incorporating additional evaluation metrics beyond semantic similarity, to provide a more comprehensive and multi-dimensional assessment of CQ generation quality, e.g. evaluating the novelty / diversity or answerability / executability","Maybe in the future, the APIs could support for rdf data dump upload and sparql execution over the rdf dump so as to evaluate the answerability / executability of the CQs","Crafting CQs is a core task in KE, and Bench4KE rightly focuses on benchmarking this. In the future, it would be great to also support ontology alignment-related tasks, as these are fundamental for enabling interoperability between knowledge bases"
10/05/2025 4.10.56,4,4,4,4,2,3,Yes,3,3,"Yes, Bench4KE could benefit from additional evaluation metrics to better capture the functional, structural, and semantic quality of automatically generated CQs. The current setup includes: Cosine Similarity, Jaccard Similarity, and GPT-4-based LLM evaluation, which is a great start. You might consider: BLEU / ROUGE / METEOR","Granular Evaluation: Add endpoints for evaluating single CQs or subsets for easier debugging and development.

Online Deployment: As noted in the tutorial, deploying Bench4KE as a web service would make it significantly more accessible, especially for non-technical users or quick testing scenarios.","Ontology Requirements Extraction: Automatically extracting purpose, scope, and functional requirements from natural language specs.
Entity Extraction Evaluation: Assessing tools that identify relevant entities from domain descriptions or competency questions.
CQ-to-SPARQL Mapping: Evaluating how well generated CQs are translated into formal queries.
"